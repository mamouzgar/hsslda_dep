---
title: "hsslda-intro"
author: "meelad amouzgar"
date: "7/5/2021"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{hsslda-intro}
  %\VignetteEngine{knitr::rmarkdown}
  \usepackage[utf8]{inputenc}
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_knit$set(root.dir = getwd())

```

Install and load the hsslda library:

You can install hsslda using:

```{r eval=FALSE}
remotes::install_github("mamouzgar/hsslda", build_vignettes = TRUE)
```



```{r, warning=FALSE, include=FALSE}
library(magrittr)
```

```{r, warning=FALSE, include=FALSE}
library(hsslda)
```

Here we read-in the example data
```{r, results = 'hide'}
Tcell_intro = read.csv("data/TcellHartmann2020_sampleData.txt",sep="\t", stringsAsFactors = T)
# Tcell_intro = data(TcellHartmann2020_sampleData)

```


```{r}
colnames(Tcell_intro)
head(Tcell_intro,3)
```

You can run hybrid subset selection on a dataset using runHSS(). 

runHSS takes 3 inputs:

1) x: a dataframe of cells (rows) and markers/genes (columns)

2) y: a vector of your class labels of interest. 

3) score.method: A scoring metric to use for feature selection, which can include: 'euclidean', 'silhouette', 'pixel.entropy', 'pixel.density', or 'custom'


Here we will run HSS using the default scoring method: euclidean distance.Note that scoring metrics like silhouette or pixel class entropy (PCE) score will require additional package dependencies. 
```{r, echo=TRUE, results = 'hide'}
channels = c('GLUT1', 'HK2', 'GAPDH', 'LDHA', 'MCT1', 'PFKFB4', 'IDH2', 'CyclinB1', 'GLUD12', 'CS', 'OGDH', 'CytC', 'ATP5A', 'S6_p', 'HIF1A')
train.x = Tcell_intro[channels]
train.y = Tcell_intro[['labels']]
hss.result = runHSS(x = train.x, y = train.y, score.method = 'euclidean')

```



The final hss.result object outputted from runHSS contains a few elements, the most important ones are highlighted below:

1) HSSscores: a table of all scores for each subsetted model.

2) ElbowPlot: visualizes the elbow plot and calculated elbow point for the optimal feature set.

3) HSS-LDA-model: The final LDA model using the optimal feature set.

4) HSS-LDA-result: The final result table containing all initially inputted markers, class labels, and linear discriminants from the opimal model.


You can visualize the elbow plot, which is ggplot configurable:
```{r}
hss.result$ElbowPlot
```

The final output object contains a merged dataframe of your markers, class labels, and newly generated LD axes
```{r}
lda.df = hss.result$`HSS-LDA-result`
head(lda.df, 3)
```


```{r, echo=FALSE}
lda.df$labels = train.y
```

```{r, echo=FALSE}
viridis.colors = c("#440154FF", "#414487FF", "#2A788EFF", "#22A884FF", "#7AD151FF", "#FDE725FF")
names(viridis.colors) = unique(lda.df$labels)
myColScale = ggplot2::scale_color_manual(values = viridis.colors)
ggplot2::ggplot(lda.df,ggplot2::aes(x = LD1, y = LD2, color = labels)) +
  ggplot2::geom_point() +
  myColScale
```

The final HSS-LDA model is also saved, and can be used like any R model.
```{r, echo=TRUE}
hss.result$`HSS-LDA-model`
```

```{r, echo=FALSE, results = 'hide'}
newdata = train.x
```

You can use this final model to apply the same dimensionality reduction to new data using makeAxes.
```{r, echo=TRUE, results = 'hide'}
lda.df.newData = makeAxes(df = newdata, co =hss.result$`HSS-LDA-model`$scaling)
```


You can also add your own custom separation metric to perform feature selection with by changing score.method = 'custom' and adding custom function. 
The custom function must take in as input:

1) x: dataframe of all your data

2) y: vector of class labels matching training data rows

3) cols: vector of column names to implement HSS-LDA


```{r eval=FALSE}
hss.resultCustom = runHSS(x = train.x, y = train.y, score.method = 'custom', custom.score.method = myCustomFunction)
```


And that's how you use LDA with Hybrid Subset Selection!











